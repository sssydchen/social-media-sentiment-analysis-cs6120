{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yifanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yifanchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yifanchen/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yifanchen/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports & setup\n",
    "\n",
    "import os, re, numpy as np, pandas as pd, tensorflow as tf\n",
    "from nltk import download as nltk_download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "'''\n",
    "Sets the seed for TensorFlow’s internal random number generator\n",
    "TensorFlow uses randomness in many places: Initializing weights, Shuffling data, Dropout layers;\n",
    "Setting this makes sure that every time running the notebook, we'll get the same model initialization and training behavior\n",
    "'''\n",
    "tf.random.set_seed(42) # common placeholder seed value\n",
    "np.random.seed(42) # helps ensure NumPy-based random behavior is the same across runs.\n",
    "\n",
    "nltk_download(\"punkt\")\n",
    "nltk_download(\"stopwords\")\n",
    "nltk_download(\"wordnet\")    \n",
    "nltk_download(\"omw-1.4\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "NEGATION_WORDS = {\"no\", \"not\", \"nor\", \"cannot\", \"can't\", \"won't\", \"n't\", \"never\"}\n",
    "CUSTOM_STOPWORDS = STOP_WORDS - NEGATION_WORDS\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    tokens = word_tokenize(text.strip())\n",
    "    tokens = [word for word in tokens if word not in CUSTOM_STOPWORDS]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14640\n"
     ]
    }
   ],
   "source": [
    "'''Load & clean'''\n",
    "\n",
    "CSV_PATH = \"Tweets.csv\" # dataset file to be tested\n",
    "\n",
    "# Read the raw dataset into a DataFrame\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# Apply the text cleaner to every tweet\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Split out features and labels\n",
    "texts  = df[\"cleaned_text\"].values\n",
    "labels = df[\"airline_sentiment\"].values\n",
    "print(\"Dataset size:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes → ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "'''Train/Test split + label encoding'''\n",
    "\n",
    "# Split so every model sees the exact same data partitions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.20, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Map string labels to integer IDs\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc  = label_encoder.transform(y_test)\n",
    "\n",
    "num_classes  = len(label_encoder.classes_)\n",
    "y_train_cat  = to_categorical(y_train_enc, num_classes)\n",
    "y_test_cat   = to_categorical(y_test_enc,  num_classes)\n",
    "\n",
    "print(\"Classes →\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (training): 9088\n"
     ]
    }
   ],
   "source": [
    "'''Tokeniser & padding'''\n",
    "\n",
    "# Hyper‑parameters for the tokeniser and sequences\n",
    "MAX_VOCAB, MAX_SEQ_LEN, EMBED_DIM = 10_000, 100, 64 # max_vocab limits the size of the vocabulary that the model is allowed to learn\n",
    "\n",
    "# Build and fit the tokeniser on train texts only \n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to integer sequences and pad sequences to uniform length\n",
    "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                            maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_test_pad  = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                            maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "print(\"Vocab size (training):\", len(tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env_py311/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │       \u001b[38;5;34m640,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">706,435</span> (2.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m706,435\u001b[0m (2.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">706,435</span> (2.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m706,435\u001b[0m (2.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Build Bidirectional-LSTM architecture '''\n",
    "\n",
    "# Define the model sequentially; add input_shape so Keras builds immediately\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB, # vocabulary cap\n",
    "              output_dim=EMBED_DIM, # embedding vector length\n",
    "              input_shape=(MAX_SEQ_LEN,)),  # one integer per time‑step\n",
    "    Bidirectional(LSTM(64)),  # 64 units forward + 64 backward\n",
    "    Dropout(0.5), # regularisation\n",
    "    Dense(num_classes, activation=\"softmax\") # 3‑way sentiment output\n",
    "])\n",
    "\n",
    "# Compile with categorical cross‑entropy & Adam optimiser\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5316628081165736, 1: 1.5748285599031868, 2: 2.0656084656084657}\n"
     ]
    }
   ],
   "source": [
    "'''Compute balanced weights for each class'''\n",
    "\n",
    "# Balanced weighting: each class contributes equally to loss\n",
    "weights = compute_class_weight(\"balanced\",\n",
    "                               classes=np.unique(y_train_enc),\n",
    "                               y=y_train_enc)\n",
    "class_weights = dict(enumerate(weights))\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "293/293 - 11s - 36ms/step - accuracy: 0.6374 - loss: 0.8351 - val_accuracy: 0.7759 - val_loss: 0.5657\n",
      "Epoch 2/15\n",
      "293/293 - 11s - 37ms/step - accuracy: 0.8082 - loss: 0.4980 - val_accuracy: 0.7840 - val_loss: 0.5452\n",
      "Epoch 3/15\n",
      "293/293 - 11s - 38ms/step - accuracy: 0.8720 - loss: 0.3496 - val_accuracy: 0.7708 - val_loss: 0.6155\n",
      "Epoch 4/15\n",
      "293/293 - 11s - 38ms/step - accuracy: 0.9056 - loss: 0.2615 - val_accuracy: 0.7589 - val_loss: 0.7028\n",
      "Epoch 5/15\n",
      "293/293 - 11s - 39ms/step - accuracy: 0.9273 - loss: 0.2109 - val_accuracy: 0.7554 - val_loss: 0.8055\n"
     ]
    }
   ],
   "source": [
    "# Train with early stopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "\tmonitor=\"val_loss\",  # watch validation loss\n",
    "\tpatience=3, # stop after 3 epochs w/o improvement\n",
    "    restore_best_weights=True) # roll back to the best epoch\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train_cat,\n",
    "    epochs=15, batch_size=32,\n",
    "    validation_split=0.2, # hold out 20 % of train as val\n",
    "    class_weight=class_weights, # handle class imbalance\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2 # 1 line/epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7760 | Test loss: 0.5550\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.84      0.86      1835\n",
      "     neutral       0.59      0.63      0.61       620\n",
      "    positive       0.68      0.70      0.69       473\n",
      "\n",
      "    accuracy                           0.78      2928\n",
      "   macro avg       0.71      0.73      0.72      2928\n",
      "weighted avg       0.78      0.78      0.78      2928\n",
      "\n",
      "Macro-F1: 0.7185641204283146\n"
     ]
    }
   ],
   "source": [
    "'''Evaluation & classification report'''\n",
    "\n",
    "# Evaluate on the reserved test partition\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Predict class indices on test set\n",
    "y_pred = model.predict(X_test_pad, verbose=0).argmax(axis=1)\n",
    "\n",
    "# Report\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test_enc, y_pred,\n",
    "                            target_names=label_encoder.classes_))\n",
    "print(\"Macro-F1:\", f1_score(y_test_enc, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved → tf_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "'''Save predictions to CSV'''\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"text\"           : X_test,\n",
    "    \"airline_sentiment\"     : label_encoder.inverse_transform(y_test_enc),\n",
    "    \"predicted_sentiment\": label_encoder.inverse_transform(y_pred)\n",
    "})\n",
    "pred_df.to_csv(\"tf_predictions_lemmatization.csv\", index=False)\n",
    "print(\"Predictions saved → tf_predictions_lemmatization.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
