{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c428eced-b12f-4610-8821-817fb77a2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (4.51.2)\n",
      "Requirement already satisfied: pandas in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kaijiesong/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/kaijiesong/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaijiesong/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdc06aa-02e3-4323-9628-15359b07c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies have been imported, and the random seed has been set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import time\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"All dependencies have been imported, and the random seed has been set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ba4830-a3b3-479e-b52b-b537a52e885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data：\n",
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
      "Label mapping： {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Data loading and preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"Tweets.csv\"  \n",
    "# df = pd.read_csv(csv_path, nrows=5000)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Sample data：\")\n",
    "print(df.head())\n",
    "\n",
    "df = df.dropna(subset=['text', 'airline_sentiment'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['airline_sentiment'])\n",
    "print(\"Label mapping：\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "print(\"Data loading and preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9bb955-caf4-453b-8afa-ad353ce7845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TweetDataset definition is complete.\n"
     ]
    }
   ],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.data.iloc[index]['text'])\n",
    "        label = int(self.data.iloc[index]['label'])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"The TweetDataset definition is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8b0c50-2b95-4e98-ba7b-5f25423ebeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete: training set samples. 11712, Validation set samples 2928\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset = TweetDataset(df, tokenizer, max_len=128)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Dataset split complete: training set samples. {train_size}, Validation set samples {val_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f211b3c-cbb1-490a-8273-2d15712fdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialization, along with the optimizer and scheduler setup, is complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaijiesong/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_encoder.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "epochs = 3  \n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"Model initialization, along with the optimizer and scheduler setup, is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115bb5a8-a4ee-4890-9966-4af24e5d0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training and validation functions have been defined.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses), all_preds, all_labels\n",
    "\n",
    "print(\"The training and validation functions have been defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d09c6981-09d8-4583-b825-6ec1bf3b7d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training model...\n",
      "Epoch 1/3\n",
      "Train loss: 0.4883, Accuracy: 0.8101\n",
      "Val loss: 0.3907, Accuracy: 0.8470\n",
      "Epoch comsumed time: 2716 seconds\n",
      "\n",
      "Best model is saved\n",
      "Epoch 2/3\n",
      "Train loss: 0.2691, Accuracy: 0.9057\n",
      "Val loss: 0.4443, Accuracy: 0.8542\n",
      "Epoch comsumed time: 2671 seconds\n",
      "\n",
      "Best model is saved\n",
      "Epoch 3/3\n",
      "Train loss: 0.1610, Accuracy: 0.9517\n",
      "Val loss: 0.5363, Accuracy: 0.8535\n",
      "Epoch comsumed time: 2834 seconds\n",
      "\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "best_accuracy = 0\n",
    "\n",
    "print(\"Begin training model...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    val_acc, val_loss, y_pred, y_true = eval_model(model, val_loader, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    history['train_acc'].append(train_acc.item())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.item())\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f'Train loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "    print(f'Val loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n",
    "    print(f'Epoch comsumed time: {end_time - start_time:.0f} seconds\\n')\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        print(\"Best model is saved\")\n",
    "\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be9b384f-87f2-4232-a6de-fcf29915a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91      1825\n",
      "     neutral       0.75      0.67      0.71       638\n",
      "    positive       0.79      0.83      0.81       465\n",
      "\n",
      "    accuracy                           0.85      2928\n",
      "   macro avg       0.81      0.81      0.81      2928\n",
      "weighted avg       0.85      0.85      0.85      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report：\")\n",
    "report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b264a19a-3eea-4046-93f8-a245131523df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save complete，first 5 row previewed：\n",
      "             tweet_id bert_pred airline_sentiment\n",
      "0  570306133677760513   neutral           neutral\n",
      "1  570301130888122368  positive          positive\n",
      "2  570301083672813571  negative           neutral\n",
      "3  570301031407624196  negative          negative\n",
      "4  570300817074462722  negative          negative\n",
      "\n",
      " comparation was saved to `bert_vs_original.csv`。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "\n",
    "for text in df['text']:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        str(text),\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    pred_label_id = torch.argmax(logits, dim=1).cpu().item()\n",
    "    pred_label_str = label_encoder.inverse_transform([pred_label_id])[0]\n",
    "    preds.append(pred_label_str)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'tweet_id': df['tweet_id'],\n",
    "    'bert_pred': preds,\n",
    "    'airline_sentiment': df['airline_sentiment']\n",
    "})\n",
    "\n",
    "output_path = \"bert_vs_original.csv\"\n",
    "output_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"save complete，first 5 row previewed：\")\n",
    "print(output_df.head())\n",
    "\n",
    "print(f\"\\n comparation was saved to `{output_path}`。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd714fad-8ad8-44a5-b984-3785b9cf2c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1bb4e-7532-4aa7-b758-2c50e573618c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21058d21-4067-4aec-b359-f5d66fb476bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c717b9-118e-4421-a74d-2e9219870436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8541aec-87b4-4f35-9e33-2b7e50db1410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535a285-3413-4f42-adb5-4fb2eb6f9cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3c443-db5c-4996-acf6-33dd2afb3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b9ac3-06ef-4296-9be8-5935cb2d9c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
